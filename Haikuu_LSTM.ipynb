{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Haikuus are usually short poems consisting of three lines.Traditionaly the 1st line is 5 sylable the second is 7 and the third is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['rectory roofers\\ntheir ladders\\ntake them higher\\n'],\n",
       "       ['summer cabin\\nthe ants\\ndo the dishes\\n'],\n",
       "       ['lagoon at sunrise?\\nthe shadow\\nchases its pelican\\n'],\n",
       "       ...,\n",
       "       ['spruce woods\\nfireweed filling\\nthe vacancy\\n'],\n",
       "       ['corrugated sun?\\nchilies and laundry\\nin rooftop haze\\n'],\n",
       "       ['home from war\\nwe ease out\\nthe champagne corks\\n']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "data = pd.read_csv(\"haikus.csv\",header=None,encoding = \"%%latin1\")\n",
    "data = data.rename({0:\"Haikuu\"},axis=1)\n",
    "#data=data.iloc[0:100,:]\n",
    "df = data.values\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data(Dividing the Haikuu into 3 Lines(Original Structure!))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub)\n",
    "pattern = '\\\\'\n",
    "count = 0\n",
    "for i in df:\n",
    "    col1=[]\n",
    "    col2=[]\n",
    "    col3=[]\n",
    "    pattern1 = ''\n",
    "    positions = []\n",
    "    my_lst_str=''\n",
    "    my_lst_str1=''\n",
    "    my_lst_str2=''\n",
    "    pattern1 = str(i).strip('[]')\n",
    "    positions = list(find_all(pattern1,pattern))\n",
    "    if (len(positions) == 3):\n",
    "        \n",
    "        for j in range(positions[0]):\n",
    "            col1.append(pattern1[j])\n",
    "            my_lst_str = ''.join(map(str, col1)) \n",
    "    \n",
    "        for j in range(positions[0]+2,positions[1]):\n",
    "            col2.append(pattern1[j])\n",
    "            my_lst_str1 = ''.join(map(str, col2))\n",
    "            \n",
    "        \n",
    "        for j in range(positions[1]+2,positions[2]):\n",
    "            col3.append(pattern1[j])\n",
    "            my_lst_str2 = ''.join(map(str, col3)) \n",
    "    elif(len(positions) == 2):\n",
    "        for j in range(positions[0]):\n",
    "            col1.append(pattern1[j])\n",
    "            my_lst_str = ''.join(map(str, col1)) \n",
    "    \n",
    "        for j in range(positions[0]+2,positions[1]):\n",
    "            col2.append(pattern1[j])\n",
    "            my_lst_str1 = ''.join(map(str, col2))\n",
    "        my_lst_str2 = '...'\n",
    "    else:\n",
    "        for j in range(positions[0]):\n",
    "            col1.append(pattern1[j])\n",
    "            my_lst_str = ''.join(map(str, col1)) \n",
    "        my_lst_str1 = '...'\n",
    "        my_lst_str2 = '...'\n",
    "    data.loc[count, '1st Verse'] = my_lst_str\n",
    "    data.loc[count, '2nd Verse'] = my_lst_str1\n",
    "    data.loc[count, '3rd Verse'] = my_lst_str2\n",
    "    count = count+1\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Haikuu</th>\n",
       "      <th>1st Verse</th>\n",
       "      <th>2nd Verse</th>\n",
       "      <th>3rd Verse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rectory roofers\\ntheir ladders\\ntake them high...</td>\n",
       "      <td>'rectory roofers</td>\n",
       "      <td>their ladders</td>\n",
       "      <td>take them higher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summer cabin\\nthe ants\\ndo the dishes\\n</td>\n",
       "      <td>'summer cabin</td>\n",
       "      <td>the ants</td>\n",
       "      <td>do the dishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lagoon at sunrise?\\nthe shadow\\nchases its pel...</td>\n",
       "      <td>'lagoon at sunrise?</td>\n",
       "      <td>the shadow</td>\n",
       "      <td>chases its pelican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>barren trees\\neven the tiniest twig\\nembraced ...</td>\n",
       "      <td>'barren trees</td>\n",
       "      <td>even the tiniest twig</td>\n",
       "      <td>embraced by the mist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>windfall apples\\nbees tango\\nto a waltz\\n</td>\n",
       "      <td>'windfall apples</td>\n",
       "      <td>bees tango</td>\n",
       "      <td>to a waltz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8365</th>\n",
       "      <td>coconut grove\\nleaf blades comb\\nthe moonlight\\n</td>\n",
       "      <td>'coconut grove</td>\n",
       "      <td>leaf blades comb</td>\n",
       "      <td>the moonlight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8366</th>\n",
       "      <td>alpine lake?\\nmy?breast'stroke's'shining?arc\\n...</td>\n",
       "      <td>\"alpine lake?</td>\n",
       "      <td>my?breast'stroke's'shining?arc</td>\n",
       "      <td>toward sunrise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8367</th>\n",
       "      <td>spruce woods\\nfireweed filling\\nthe vacancy\\n</td>\n",
       "      <td>'spruce woods</td>\n",
       "      <td>fireweed filling</td>\n",
       "      <td>the vacancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8368</th>\n",
       "      <td>corrugated sun?\\nchilies and laundry\\nin rooft...</td>\n",
       "      <td>'corrugated sun?</td>\n",
       "      <td>chilies and laundry</td>\n",
       "      <td>in rooftop haze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>home from war\\nwe ease out\\nthe champagne corks\\n</td>\n",
       "      <td>'home from war</td>\n",
       "      <td>we ease out</td>\n",
       "      <td>the champagne corks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8370 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Haikuu            1st Verse  \\\n",
       "0     rectory roofers\\ntheir ladders\\ntake them high...     'rectory roofers   \n",
       "1               summer cabin\\nthe ants\\ndo the dishes\\n        'summer cabin   \n",
       "2     lagoon at sunrise?\\nthe shadow\\nchases its pel...  'lagoon at sunrise?   \n",
       "3     barren trees\\neven the tiniest twig\\nembraced ...        'barren trees   \n",
       "4             windfall apples\\nbees tango\\nto a waltz\\n     'windfall apples   \n",
       "...                                                 ...                  ...   \n",
       "8365   coconut grove\\nleaf blades comb\\nthe moonlight\\n       'coconut grove   \n",
       "8366  alpine lake?\\nmy?breast'stroke's'shining?arc\\n...        \"alpine lake?   \n",
       "8367      spruce woods\\nfireweed filling\\nthe vacancy\\n        'spruce woods   \n",
       "8368  corrugated sun?\\nchilies and laundry\\nin rooft...     'corrugated sun?   \n",
       "8369  home from war\\nwe ease out\\nthe champagne corks\\n       'home from war   \n",
       "\n",
       "                           2nd Verse             3rd Verse  \n",
       "0                      their ladders      take them higher  \n",
       "1                           the ants         do the dishes  \n",
       "2                         the shadow    chases its pelican  \n",
       "3              even the tiniest twig  embraced by the mist  \n",
       "4                         bees tango            to a waltz  \n",
       "...                              ...                   ...  \n",
       "8365                leaf blades comb         the moonlight  \n",
       "8366  my?breast'stroke's'shining?arc        toward sunrise  \n",
       "8367                fireweed filling           the vacancy  \n",
       "8368             chilies and laundry       in rooftop haze  \n",
       "8369                     we ease out   the champagne corks  \n",
       "\n",
       "[8370 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haikuu_list=[]\n",
    "haikuu_string =''\n",
    "\n",
    "df = data.iloc[:,1:].values\n",
    "for i in range(len(df)):\n",
    "    for j in range(0,3):\n",
    "        haikuu_list.append(df[i][j])\n",
    "        haikuu_string = ', '.join(map(str, haikuu_list))\n",
    "haikuu_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger','ner'])\n",
    "\n",
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\'\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']\n",
    "\n",
    "tokens = separate_punc(haikuu_string)\n",
    "       http://localhost:8888/notebooks/06-Deep-Learning/Untitled.ipynb# \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Sequences of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "train_len = 3+1 # 50 training words , then one target word\n",
    "\n",
    "# Empty list of sequences\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    \n",
    "    # Grab train_len# amount of characters\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    # Add to list of sequences\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sequences[]:\n",
    "    print(f'{i} : {tokenizer.index_word[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sequences = np.array(sequences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model:LSTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 3, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "X= sequences[:,:-1]\n",
    "Y = sequences[:,-1]\n",
    "seq_len = X.shape[1]\n",
    "Y = to_categorical(Y, num_classes=vocabulary_size+1)\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # define model\n",
    "model = create_model(vocabulary_size+1, seq_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load\n",
    "# fit model\n",
    "model.fit(X, Y, batch_size=128, epochs=300,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "#    INPUTS:\n",
    "#    model : model that was trained on text data\n",
    "#    tokenizer : tokenizer that was fit on text data\n",
    "#    seq_len : length of training sequence\n",
    "#    seed_text : raw string text to serve as the seed\n",
    "#    num_gen_words : number of words to be generated by model\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Intial sentence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create requested number of words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        # Take the input text string and encode it to a sequence\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        # Pad sequences to our trained rate(making it robust)\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        # Predict Class Probabilities for each word\n",
    "        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n",
    "        \n",
    "        # Grab word\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "        \n",
    "        # Update the sequence of input text (shifting one over with the new word)\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    # Making it look like a sentence.\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grab a random seed sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0,len(text_sequences))\n",
    "random_seed_text = text_sequences[random_pick]\n",
    "seed_text = ' '.join(random_seed_text)\n",
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
